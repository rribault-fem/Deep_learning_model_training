# Deep_learning_model_training
# Surrogate Model Training
This script trains a surrogate model using the specified configuration.

## Configuration
The configuration is stored in a YAML file and can be modified to change the behavior of the training process. 
The management of YAML file is based on the hydra package : https://hydra.cc/docs/intro/
Please perform the structured config hydra tutorial before using this package.
https://hydra.cc/docs/1.0/tutorials/structured_config/intro/

## Set up your own env.yaml file


## Outputs
The trained model, logs, experiment configuration and environments used during training are saved in the `outputs` directory.
One subfolder is created for each taskname define in the config.yaml file.
For each experiment, a subfolder is created with the current date and time.
This behavior is defined in the configs/hydra/default.yaml file.

<br>
## Project Structure

The directory structure of new project looks like this:

```
├── .github                   <- Github Actions workflows
│
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── data                     <- Ligthning DataModule configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs (Tensorboad...)
│   ├── model                    <- Model configs
│   ├── preprocessing            <- Project preprocessing configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── data                   <- Project data
│
├── outputs                   <- Logs generated by hydra and lightning loggers
│
├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
│                             the creator's initials, and a short `-` delimited description,
│                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
│
├── scripts                <- Shell scripts
│
├── src                    <- Source code
│   ├── data                     <- Data scripts
│   ├── models                   <- Model scripts
│   ├── utils                    <- Utility scripts
│   ├── preprocessing            <- Preprocessing scripts
│   │
│   ├── eval.py                  <- Run evaluation
│   └── train.py                 <- Run training
│
├── tests                  <- Tests of any kind
│
├── .env.example              <- Example of file for storing private environment variables
├── .gitignore                <- List of files ignored by git
├── .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
├── .project-root             <- File for inferring the position of project root directory
├── environment.yaml          <- File for installing conda environment
├── Makefile                  <- Makefile with commands like `make train` or `make test`
├── pyproject.toml            <- Configuration options for testing and linting
├── requirements.txt          <- File for installing python dependencies
├── setup.py                  <- File for installing project as a package
└── README.md
```

<br>

## Usage
To run this script, use the following command: `python train_surrogate.py`
This will train a surrogate model using the specified configuration and save it for future use.

## Recover shared models and datasets
To recover the shared models and datasets, you need to download the data from DVC.
run DVC pull in the project root directory.

## Pre-processing

The `Pre_process_data` function is used to pre-process the data before training. It loads the dataset and performs various operations such as dropping missing values and rearranging direction columns.

## Training

The `Train_model` function is used to train the surrogate model. It imports the specified model type and trains it on the pre-processed data.

## Saving

After training, both the pipeline and trained model are saved for future use.

## INFER

Check path definition in env.yaml
run python surrogate_inference.py



## Configuration example
Several scaler and decomposition methods are available : all scalers from scikit-learn and the decomposition methods from scikit-learn. 
for example you can change the environmental scaler type, edit the `scaler_options` of the envir_scaler object to any of the following:
- StandardScaler
- MinMaxScaler
- MaxAbsScaler
- ...

If required you can also implement you own scaler method in envir_scaler.py.

To split a dataset into a test and training data set you can either used the proposed `find_test_set_in_model_validity_domain` or write another method in split_transform.py and set the method name in `split_method` parameter of `config.yaml` file.

To change the configuration, edit the `config.yaml` file.
--> To change a model type, edit the `model_type` parameter.

